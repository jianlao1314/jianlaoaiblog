import requests
import os
import time
from xml.etree import ElementTree as ET
from urllib.parse import urljoin

# é…ç½®éƒ¨åˆ†
BASE_URL = "https://ai.5334427.xyz"
SITEMAP_INDEX = f"{BASE_URL}/sitemap-index.xml"
BAIDU_PUSH_API = "http://data.zz.baidu.com/urls?site=https://ai.5334427.xyz&token=r8yEgZ0WpYoP70Av"
URLS_FILE = "urls.txt"
BACKUP_FILE = "urls.txt.1"
MAX_FILE_SIZE = 100 * 1024  # 100KB

def fetch_sitemap_index():
    """è·å–ç«™ç‚¹åœ°å›¾ç´¢å¼•æ–‡ä»¶"""
    print("ğŸ“¥ æ­£åœ¨æŠ“å–ç«™ç‚¹åœ°å›¾ç´¢å¼•:", SITEMAP_INDEX)
    try:
        resp = requests.get(SITEMAP_INDEX)
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
        # è·å–æ‰€æœ‰ sitemap æ–‡ä»¶çš„ URL
        sitemap_urls = [el.text for el in root.findall(".//{*}loc")]
        print(f"ğŸ“‹ æ‰¾åˆ° {len(sitemap_urls)} ä¸ªç«™ç‚¹åœ°å›¾æ–‡ä»¶")
        return sitemap_urls
    except Exception as e:
        print("âŒ è·å–ç«™ç‚¹åœ°å›¾ç´¢å¼•å¤±è´¥:", e)
        return []

def extract_urls_from_sitemap(sitemap_url):
    """ä»å•ä¸ªç«™ç‚¹åœ°å›¾æ–‡ä»¶ä¸­æå– URL"""
    print("ğŸ” åˆ†æç«™ç‚¹åœ°å›¾ï¼š", sitemap_url)
    urls = []
    try:
        resp = requests.get(sitemap_url)
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
        # è·å–æ‰€æœ‰ URL
        for loc in root.findall(".//{*}loc"):
            url = loc.text.strip()
            if "/article/" in url:  # åªå¤„ç†æ–‡ç« é¡µé¢
                urls.append(url)
        print(f"âœ… ä» {sitemap_url} ä¸­æå–äº† {len(urls)} ä¸ªæ–‡ç«  URL")
    except Exception as e:
        print("âŒ å¤„ç†ç«™ç‚¹åœ°å›¾å¤±è´¥:", sitemap_url, e)
    return urls

def load_processed_urls():
    """åŠ è½½å·²å¤„ç†è¿‡çš„ URL"""
    if not os.path.exists(BACKUP_FILE):
        return set()
    try:
        with open(BACKUP_FILE, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f)
    except Exception as e:
        print("âŒ è¯»å–å†å²è®°å½•å¤±è´¥:", e)
        return set()

def push_to_baidu(urls):
    """æ¨é€ URL åˆ°ç™¾åº¦"""
    if not urls:
        print("âœ… æ²¡æœ‰æ–°çš„ URL éœ€è¦æäº¤")
        return

    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œç¡®ä¿æ¯è¡Œç»“å°¾æ˜¯ \n
    with open(URLS_FILE, "w", encoding="utf-8", newline='\n') as f:
        f.write("\n".join(urls) + "\n")  # ç¡®ä¿æœ€åä¸€è¡Œä¹Ÿæœ‰æ¢è¡Œç¬¦

    print(f"ğŸš€ æ­£åœ¨æäº¤ {len(urls)} ä¸ª URL åˆ°ç™¾åº¦")
    try:
        # å®Œå…¨æ¨¡æ‹Ÿ curl å‘½ä»¤
        headers = {
            'Content-Type': 'text/plain',
            'User-Agent': 'curl/7.64.1'  # æ¨¡æ‹Ÿ curl
        }
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(URLS_FILE, 'rb') as f:
            content = f.read()
            
        # å‘é€è¯·æ±‚
        resp = requests.post(
            BAIDU_PUSH_API,
            headers=headers,
            data=content,
            timeout=30
        )
        
        print("ğŸ“¨ ç™¾åº¦å“åº”ï¼š", resp.text)
        
        # æ£€æŸ¥å“åº”
        if resp.status_code == 200:
            print("âœ… æ¨é€æˆåŠŸ")
            # æ›´æ–°å†å²è®°å½•
            if os.path.getsize(URLS_FILE) > MAX_FILE_SIZE:
                print("ğŸ“¦ æ–‡ä»¶è¿‡å¤§ï¼Œå¤‡ä»½ä¸º:", BACKUP_FILE)
                os.replace(URLS_FILE, BACKUP_FILE)
            else:
                with open(BACKUP_FILE, "a", encoding="utf-8", newline='\n') as f:
                    for url in urls:
                        f.write(url + "\n")
        else:
            print(f"âŒ æ¨é€å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{resp.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯ï¼š{resp.text}")
            
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print("\nè°ƒè¯•ä¿¡æ¯:")
            print(f"URL æ•°é‡: {len(urls)}")
            print(f"æ–‡ä»¶å¤§å°: {len(content)} å­—èŠ‚")
            print(f"ç¬¬ä¸€ä¸ª URL: {urls[0] if urls else 'None'}")
            
    except Exception as e:
        print("âŒ æ¨é€å¤±è´¥:", e)

def main():
    try:
        # 1. è·å–ç«™ç‚¹åœ°å›¾ç´¢å¼•
        sitemap_urls = fetch_sitemap_index()
        if not sitemap_urls:
            print("â—æœªæ‰¾åˆ°ç«™ç‚¹åœ°å›¾æ–‡ä»¶")
            return

        # 2. ä»æ‰€æœ‰ç«™ç‚¹åœ°å›¾ä¸­æå– URL
        all_urls = set()
        for sitemap_url in sitemap_urls:
            urls = extract_urls_from_sitemap(sitemap_url)
            all_urls.update(urls)

        print(f"ğŸ“Š æ€»å…±å‘ç° {len(all_urls)} ä¸ªæ–‡ç« é¡µé¢")

        # 3. è·å–å·²å¤„ç†çš„ URL
        processed_urls = load_processed_urls()
        print(f"ğŸ“š å†å²è®°å½•ä¸­æœ‰ {len(processed_urls)} ä¸ªå·²å¤„ç†çš„ URL")

        # 4. æ‰¾å‡ºæ–°çš„ URL
        new_urls = sorted(all_urls - processed_urls)
        print(f"ğŸ†• å‘ç° {len(new_urls)} ä¸ªæ–°çš„ URL")

        # 5. æ¨é€åˆ°ç™¾åº¦
        push_to_baidu(new_urls)

    except Exception as e:
        print("â—ç¨‹åºè¿è¡Œå¤±è´¥ï¼š", e)

if __name__ == "__main__":
    main()
