import requests
import os
import time
from xml.etree import ElementTree as ET
from urllib.parse import urljoin

# é…ç½®éƒ¨åˆ†
BASE_URL = "https://ai.5334427.xyz"
SITEMAP_INDEX = f"{BASE_URL}/sitemap-index.xml"
BAIDU_PUSH_API = "http://data.zz.baidu.com/urls?site=https://ai.5334427.xyz&token=r8yEgZ0WpYoP70Av"
INDEXNOW_API = "https://api.indexnow.org/IndexNow"
INDEXNOW_KEY = "04412298a9d047acb5eb9d71ba35e590"
URLS_FILE = "urls.txt"
BACKUP_FILE = "urls.txt.1"
MAX_FILE_SIZE = 100 * 1024  # 100KB

def fetch_sitemap_index():
    """è·å–ç«™ç‚¹åœ°å›¾ç´¢å¼•æ–‡ä»¶"""
    print("ğŸ“¥ æ­£åœ¨æŠ“å–ç«™ç‚¹åœ°å›¾ç´¢å¼•:", SITEMAP_INDEX)
    try:
        resp = requests.get(SITEMAP_INDEX)
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
        # è·å–æ‰€æœ‰ sitemap æ–‡ä»¶çš„ URL
        sitemap_urls = [el.text for el in root.findall(".//{*}loc")]
        print(f"ğŸ“‹ æ‰¾åˆ° {len(sitemap_urls)} ä¸ªç«™ç‚¹åœ°å›¾æ–‡ä»¶")
        return sitemap_urls
    except Exception as e:
        print("âŒ è·å–ç«™ç‚¹åœ°å›¾ç´¢å¼•å¤±è´¥:", e)
        return []

def extract_urls_from_sitemap(sitemap_url):
    """ä»å•ä¸ªç«™ç‚¹åœ°å›¾æ–‡ä»¶ä¸­æå– URL"""
    print("ğŸ” åˆ†æç«™ç‚¹åœ°å›¾ï¼š", sitemap_url)
    urls = []
    try:
        resp = requests.get(sitemap_url)
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
        # è·å–æ‰€æœ‰ URL
        for loc in root.findall(".//{*}loc"):
            url = loc.text.strip()
            if "/article/" in url:  # åªå¤„ç†æ–‡ç« é¡µé¢
                urls.append(url)
        print(f"âœ… ä» {sitemap_url} ä¸­æå–äº† {len(urls)} ä¸ªæ–‡ç«  URL")
    except Exception as e:
        print("âŒ å¤„ç†ç«™ç‚¹åœ°å›¾å¤±è´¥:", sitemap_url, e)
    return urls

def load_processed_urls():
    """åŠ è½½å·²å¤„ç†è¿‡çš„ URL"""
    if not os.path.exists(BACKUP_FILE):
        return set()
    try:
        with open(BACKUP_FILE, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f)
    except Exception as e:
        print("âŒ è¯»å–å†å²è®°å½•å¤±è´¥:", e)
        return set()

def push_to_baidu(urls):
    """æ¨é€ URL åˆ°ç™¾åº¦"""
    if not urls:
        print("âœ… æ²¡æœ‰æ–°çš„ URL éœ€è¦æäº¤åˆ°ç™¾åº¦")
        return

    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œç¡®ä¿æ¯è¡Œç»“å°¾æ˜¯ \n
    with open(URLS_FILE, "w", encoding="utf-8", newline='\n') as f:
        f.write("\n".join(urls) + "\n")

    print(f"ğŸš€ æ­£åœ¨æäº¤ {len(urls)} ä¸ª URL åˆ°ç™¾åº¦")
    try:
        headers = {
            'Content-Type': 'text/plain',
            'User-Agent': 'curl/7.64.1'
        }
        
        with open(URLS_FILE, 'rb') as f:
            content = f.read()
            
        resp = requests.post(
            BAIDU_PUSH_API,
            headers=headers,
            data=content,
            timeout=30
        )
        
        print("ğŸ“¨ ç™¾åº¦å“åº”ï¼š", resp.text)
        
        if resp.status_code == 200:
            print("âœ… ç™¾åº¦æ¨é€æˆåŠŸ")
            return True
        else:
            print(f"âŒ ç™¾åº¦æ¨é€å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{resp.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯ï¼š{resp.text}")
            return False
            
    except Exception as e:
        print("âŒ ç™¾åº¦æ¨é€å¤±è´¥:", e)
        return False

def push_to_bing(urls):
    """æ¨é€ URL åˆ° Bing (IndexNow)"""
    if not urls:
        print("âœ… æ²¡æœ‰æ–°çš„ URL éœ€è¦æäº¤åˆ° Bing")
        return

    print(f"ğŸš€ æ­£åœ¨æäº¤ {len(urls)} ä¸ª URL åˆ° Bing")
    try:
        headers = {
            'Content-Type': 'application/json; charset=utf-8'
        }
        
        data = {
            "host": BASE_URL.replace("https://", ""),
            "key": INDEXNOW_KEY,
            "keyLocation": f"{BASE_URL}/{INDEXNOW_KEY}.txt",
            "urlList": urls
        }
        
        resp = requests.post(
            INDEXNOW_API,
            headers=headers,
            json=data,
            timeout=30
        )
        
        print("ğŸ“¨ Bing å“åº”ï¼š", resp.json())
        
        if resp.status_code == 200:
            print("âœ… Bing æ¨é€æˆåŠŸ")
            return True
        else:
            print(f"âŒ Bing æ¨é€å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{resp.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯ï¼š{resp.text}")
            return False
            
    except Exception as e:
        print("âŒ Bing æ¨é€å¤±è´¥:", e)
        return False

def update_history(urls):
    """æ›´æ–°å†å²è®°å½•"""
    if os.path.getsize(URLS_FILE) > MAX_FILE_SIZE:
        print("ğŸ“¦ æ–‡ä»¶è¿‡å¤§ï¼Œå¤‡ä»½ä¸º:", BACKUP_FILE)
        os.replace(URLS_FILE, BACKUP_FILE)
    else:
        with open(BACKUP_FILE, "a", encoding="utf-8", newline='\n') as f:
            for url in urls:
                f.write(url + "\n")

def main():
    try:
        # 1. è·å–ç«™ç‚¹åœ°å›¾ç´¢å¼•
        sitemap_urls = fetch_sitemap_index()
        if not sitemap_urls:
            print("â—æœªæ‰¾åˆ°ç«™ç‚¹åœ°å›¾æ–‡ä»¶")
            return

        # 2. ä»æ‰€æœ‰ç«™ç‚¹åœ°å›¾ä¸­æå– URL
        all_urls = set()
        for sitemap_url in sitemap_urls:
            urls = extract_urls_from_sitemap(sitemap_url)
            all_urls.update(urls)

        print(f"ğŸ“Š æ€»å…±å‘ç° {len(all_urls)} ä¸ªæ–‡ç« é¡µé¢")

        # 3. è·å–å·²å¤„ç†çš„ URL
        processed_urls = load_processed_urls()
        print(f"ğŸ“š å†å²è®°å½•ä¸­æœ‰ {len(processed_urls)} ä¸ªå·²å¤„ç†çš„ URL")

        # 4. æ‰¾å‡ºæ–°çš„ URL
        new_urls = sorted(all_urls - processed_urls)
        print(f"ğŸ†• å‘ç° {len(new_urls)} ä¸ªæ–°çš„ URL")

        if new_urls:
            # 5. æ¨é€åˆ°ç™¾åº¦
            baidu_success = ""#push_to_baidu(new_urls)
            
            # 6. æ¨é€åˆ° Bing
            bing_success = push_to_bing(new_urls)
            
            # 7. å¦‚æœä»»ä¸€æ¨é€æˆåŠŸï¼Œæ›´æ–°å†å²è®°å½•
            if baidu_success or bing_success:
                update_history(new_urls)

    except Exception as e:
        print("â—ç¨‹åºè¿è¡Œå¤±è´¥ï¼š", e)

if __name__ == "__main__":
    main()
